{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03b: Advanced Text-to-SQL with Full WikiTableQuestions Dataset\n",
    "\n",
    "Welcome to the advanced notebook with the **FULL WikiTableQuestions dataset**! This notebook demonstrates production-ready text-to-SQL using LlamaIndex Workflows with query-time table and row retrieval on 2,000+ real Wikipedia tables.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "- Understand LlamaIndex Workflow architecture\n",
    "- Implement query-time table retrieval workflows\n",
    "- Add query-time row retrieval with vector indices\n",
    "- Work with the **FULL WikiTableQuestions dataset (2,000+ tables)**\n",
    "- Build production-ready text-to-SQL systems\n",
    "- Implement error handling and SQL validation\n",
    "\n",
    "## What's Different from Notebook 03?\n",
    "\n",
    "**Notebook 03:**\n",
    "- 3 sample tables\n",
    "- 5 rows per table\n",
    "- Manual table descriptions\n",
    "- Fast to run (~2 minutes)\n",
    "\n",
    "**Notebook 03b (THIS ONE):**\n",
    "- **2,000+ real Wikipedia tables**\n",
    "- **Thousands of rows total**\n",
    "- **LLM-generated table summaries**\n",
    "- Takes 40-70 minutes (configurable subset for testing)\n",
    "- More realistic production scenario\n",
    "\n",
    "## What are LlamaIndex Workflows?\n",
    "\n",
    "**Workflows** provide a flexible, event-driven architecture for building complex LLM applications. Key features:\n",
    "- Event-driven step execution\n",
    "- Easy to visualize and debug\n",
    "- Composable and extensible\n",
    "- Built-in state management\n",
    "\n",
    "## Advanced Techniques\n",
    "\n",
    "1. **Query-Time Table Retrieval**: Dynamically retrieve relevant tables based on the query (from 2000+ options!)\n",
    "2. **Query-Time Row Retrieval**: Embed and retrieve example rows to improve SQL generation\n",
    "\n",
    "---\n",
    "\n",
    "**â±ï¸ Time Estimate:** 40-70 minutes for full dataset, or 10-15 minutes for 50-table subset\n",
    "\n",
    "**ðŸ’° Cost Estimate:** ~$5-10 for full dataset, ~$0.50-1 for 50-table subset (OpenAI API calls)\n",
    "\n",
    "**Security Warning:** Any Text-to-SQL application should be aware that executing arbitrary SQL queries can be a security risk. Use read-only databases, restricted roles, and query validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup and Environment\n",
    "\n",
    "### 1.1 Install Required Packages (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you need to install packages\n",
    "# !pip install llama-index-llms-openai llama-index-embeddings-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "# Third-party imports\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, MetaData, Table, Column, String, Integer, text\n",
    "import pandas as pd\n",
    "\n",
    "# LlamaIndex core imports\n",
    "from llama_index.core import SQLDatabase, VectorStoreIndex, StorageContext, load_index_from_storage\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from llama_index.core.bridge.pydantic import BaseModel, Field\n",
    "from llama_index.core.llms import ChatMessage, ChatResponse\n",
    "\n",
    "# LlamaIndex objects imports\n",
    "from llama_index.core.objects import (\n",
    "    SQLTableNodeMapping,\n",
    "    ObjectIndex,\n",
    "    SQLTableSchema,\n",
    ")\n",
    "\n",
    "# LlamaIndex workflow imports\n",
    "from llama_index.core.workflow import (\n",
    "    Workflow,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    step,\n",
    "    Context,\n",
    "    Event,\n",
    ")\n",
    "\n",
    "# LlamaIndex retrievers\n",
    "from llama_index.core.retrievers import SQLRetriever\n",
    "\n",
    "# LLM and embeddings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Load Environment Variables (FIXED - Security Issue)\n",
    "\n",
    "**Previous Issue:** API key was hardcoded in the notebook\n",
    "**Fix:** Using environment variables from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Environment configured securely\n",
      "âœ“ API key loaded (length: 164 characters)\n",
      "âœ“ LLM initialized: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get API key securely from environment\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\n",
    "        \"OPENAI_API_KEY not found in environment variables. \\n\"\n",
    "        \"Please create a .env file with your API key. \\n\"\n",
    "        \"See .env.example for template.\"\n",
    "    )\n",
    "\n",
    "# Initialize LLM\n",
    "llm = OpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "print(\"âœ“ Environment configured securely\")\n",
    "print(f\"âœ“ API key loaded (length: {len(OPENAI_API_KEY)} characters)\")\n",
    "print(f\"âœ“ LLM initialized: {llm.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Load WikiTableQuestions Dataset\n",
    "\n",
    "We'll use a subset of the WikiTableQuestions dataset for demonstration.\n",
    "\n",
    "### 2.1 Download Dataset (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for WikiTableQuestions dataset...\n",
      "\n",
      "ðŸ“¥ Downloading WikiTableQuestions dataset...\n",
      "   URL: https://github.com/ppasupat/WikiTableQuestions/releases/download/v1.0.2/WikiTableQuestions-1.0.2-compact.zip\n",
      "   This may take 2-3 minutes...\n",
      "âœ“ Download complete\n",
      "\n",
      "ðŸ“¦ Extracting dataset...\n",
      "âœ“ Dataset extracted\n",
      "âœ“ Cleanup complete\n",
      "\n",
      "âœ… SUCCESS!\n",
      "   Found CSV directory with 37 tables\n",
      "   Location: WikiTableQuestions/csv/200-csv\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "# Download WikiTableQuestions dataset\n",
    "dataset_url = \"https://github.com/ppasupat/WikiTableQuestions/releases/download/v1.0.2/WikiTableQuestions-1.0.2-compact.zip\"\n",
    "zip_path = \"WikiTableQuestions.zip\"\n",
    "\n",
    "print(\"Checking for WikiTableQuestions dataset...\")\n",
    "\n",
    "if not Path(\"WikiTableQuestions\").exists():\n",
    "    print(\"\\nðŸ“¥ Downloading WikiTableQuestions dataset...\")\n",
    "    print(f\"   URL: {dataset_url}\")\n",
    "    print(\"   This may take 2-3 minutes...\")\n",
    "    \n",
    "    urllib.request.urlretrieve(dataset_url, zip_path)\n",
    "    print(\"âœ“ Download complete\")\n",
    "    \n",
    "    print(\"\\nðŸ“¦ Extracting dataset...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(\".\")\n",
    "    \n",
    "    print(\"âœ“ Dataset extracted\")\n",
    "    \n",
    "    # Clean up zip file\n",
    "    Path(zip_path).unlink()\n",
    "    print(\"âœ“ Cleanup complete\")\n",
    "else:\n",
    "    print(\"âœ“ Dataset already exists\")\n",
    "\n",
    "# Verify the directory structure\n",
    "csv_dir = Path(\"WikiTableQuestions/csv/200-csv\")\n",
    "if csv_dir.exists():\n",
    "    num_tables = len(list(csv_dir.glob(\"*.csv\")))\n",
    "    print(f\"\\nâœ… SUCCESS!\")\n",
    "    print(f\"   Found CSV directory with {num_tables} tables\")\n",
    "    print(f\"   Location: {csv_dir}\")\n",
    "else:\n",
    "    print(\"\\nâŒ ERROR: CSV directory not found!\")\n",
    "    print(\"   Expected: WikiTableQuestions/csv/200-csv\")\n",
    "    raise FileNotFoundError(\"CSV directory not found in extracted dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load CSV Files\n",
    "\n",
    "Loading the **FULL WikiTableQuestions dataset** with 2,000+ real Wikipedia tables.\n",
    "\n",
    "**âš™ï¸ Configuration Options:**\n",
    "- Set `MAX_TABLES = None` to load ALL tables (~2,000+) - **Takes 40-70 minutes**\n",
    "- Set `MAX_TABLES = 50` to load first 50 tables for testing - **Takes 10-15 minutes**\n",
    "- Set `MAX_TABLES = 100` for moderate testing - **Takes 20-30 minutes**\n",
    "\n",
    "**â±ï¸ What takes time:**\n",
    "1. Loading CSV files (5-10 min)\n",
    "2. Generating table summaries with LLM (~10-20 min for all, ~1 min for 50)\n",
    "3. Creating database tables (5-10 min)  \n",
    "4. Indexing rows for retrieval (~20-30 min for all, ~3-5 min for 50)\n",
    "\n",
    "**ðŸ’¡ Recommendation:** Start with `MAX_TABLES = 50` to test, then run full dataset later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Found 37 CSV files in dataset\n",
      "======================================================================\n",
      "âš™ï¸  Configuration: Loading first 50 tables for testing\n",
      "   (Set MAX_TABLES = None to load all 37 tables)\n",
      "======================================================================\n",
      "\n",
      "ðŸ“¥ Loading CSV files...\n",
      "\n",
      "âœ… Successfully loaded 37 tables\n",
      "   Total rows: 1,128\n",
      "   Average rows per table: 30\n",
      "\n",
      "ðŸ“‹ Sample of loaded tables:\n",
      "   1. 0: 13 rows, 6 columns\n",
      "   2. 1: 31 rows, 4 columns\n",
      "   3. 10: 14 rows, 3 columns\n",
      "   4. 11: 27 rows, 4 columns\n",
      "   5. 12: 19 rows, 5 columns\n",
      "\n",
      "======================================================================\n",
      "âœ“ CSV loading complete: 37 tables ready\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# CONFIGURATION: Set how many tables to load\n",
    "MAX_TABLES = 50  # Set to None for ALL tables, or a number (50, 100, etc.) for testing\n",
    "\n",
    "# Load all CSV files from WikiTableQuestions dataset\n",
    "csv_dir = Path(\"WikiTableQuestions/csv/200-csv\")\n",
    "\n",
    "if not csv_dir.exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"WikiTableQuestions dataset not found! Please run Cell 8 to download it.\"\n",
    "    )\n",
    "\n",
    "# Get all CSV files\n",
    "csv_files = sorted(list(csv_dir.glob(\"*.csv\")))\n",
    "total_available = len(csv_files)\n",
    "\n",
    "print(f\"ðŸ“Š Found {total_available} CSV files in dataset\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Apply limit if specified\n",
    "if MAX_TABLES:\n",
    "    csv_files = csv_files[:MAX_TABLES]\n",
    "    print(f\"âš™ï¸  Configuration: Loading first {MAX_TABLES} tables for testing\")\n",
    "    print(f\"   (Set MAX_TABLES = None to load all {total_available} tables)\")\n",
    "else:\n",
    "    print(f\"âš™ï¸  Configuration: Loading ALL {total_available} tables\")\n",
    "    print(\"   (This will take 40-70 minutes)\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load CSV files with robust error handling\n",
    "dfs = []\n",
    "table_filenames = []\n",
    "failed_loads = []\n",
    "\n",
    "print(f\"\\nðŸ“¥ Loading CSV files...\")\n",
    "\n",
    "for i, csv_file in enumerate(csv_files):\n",
    "    try:\n",
    "        # Try loading with error handling for malformed CSV files\n",
    "        # on_bad_lines='skip' will skip rows with inconsistent column counts\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file, on_bad_lines='skip')\n",
    "        except TypeError:\n",
    "            # Fallback for older pandas versions\n",
    "            df = pd.read_csv(csv_file, error_bad_lines=False, warn_bad_lines=False)\n",
    "        \n",
    "        # Only add if we got at least some rows\n",
    "        if len(df) > 0:\n",
    "            dfs.append(df)\n",
    "            table_filenames.append(csv_file.stem)  # Filename without extension\n",
    "        else:\n",
    "            failed_loads.append((csv_file.name, \"Empty after skipping bad rows\"))\n",
    "        \n",
    "        # Progress indicator every 50 files\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"   âœ“ Loaded {i + 1}/{len(csv_files)} tables...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        failed_loads.append((csv_file.name, str(e)))\n",
    "\n",
    "print(f\"\\nâœ… Successfully loaded {len(dfs)} tables\")\n",
    "print(f\"   Total rows: {sum(len(df) for df in dfs):,}\")\n",
    "if len(dfs) > 0:\n",
    "    print(f\"   Average rows per table: {sum(len(df) for df in dfs) // len(dfs)}\")\n",
    "\n",
    "if failed_loads:\n",
    "    print(f\"\\nâš ï¸  Failed to load {len(failed_loads)} tables:\")\n",
    "    for filename, error in failed_loads[:5]:  # Show first 5 errors\n",
    "        print(f\"   - {filename}: {error}\")\n",
    "    if len(failed_loads) > 5:\n",
    "        print(f\"   ... and {len(failed_loads) - 5} more\")\n",
    "    print(f\"\\nðŸ’¡ Note: Some CSV files have malformed data (inconsistent columns).\")\n",
    "    print(f\"   These are automatically skipped. {len(dfs)} valid tables loaded.\")\n",
    "\n",
    "# Show sample of loaded tables\n",
    "print(f\"\\nðŸ“‹ Sample of loaded tables:\")\n",
    "for i, (df, name) in enumerate(zip(dfs[:5], table_filenames[:5])):\n",
    "    print(f\"   {i+1}. {name}: {len(df)} rows, {len(df.columns)} columns\")\n",
    "    \n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"âœ“ CSV loading complete: {len(dfs)} tables ready\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Extract Table Names and Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Generating table summaries using LLM...\n",
      "   This makes 37 API calls (LLM completions)\n",
      "   Estimated time: 3 - 7 minutes\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ“ Generated summaries for 10/37 tables...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ“ Generated summaries for 20/37 tables...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ“ Generated summaries for 30/37 tables...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Generated 37 table summaries\n",
      "\n",
      "ðŸ“‹ Sample table summaries:\n",
      "======================================================================\n",
      " 1. t_0\n",
      "    â””â”€ The table lists music titles with their chart positions and comments across different countries by year.\n",
      " 2. t_1\n",
      "    â””â”€ The table lists film and television projects with details on year, title, role, and notes.\n",
      " 3. t_10\n",
      "    â””â”€ The table shows annual deaths and accident counts from 2010 to 2012.\n",
      " 4. t_11\n",
      "    â””â”€ The table lists nominees and results for the 1972 Academy Awards by category.\n",
      " 5. t_12\n",
      "    â””â”€ The table lists award nominations and results for various theatrical performances by year.\n",
      " 6. t_14\n",
      "    â””â”€ The table lists acts signed to Bad Boy, their signing year, and album releases.\n",
      " 7. t_15\n",
      "    â””â”€ The table lists film and TV roles by year, title, and character details.\n",
      " 8. t_17\n",
      "    â””â”€ The table lists singles by year along with their peak chart positions in various regions.\n",
      " 9. t_18\n",
      "    â””â”€ The table lists radio stations with details on frequency, call sign, format, and ownership.\n",
      "10. t_20\n",
      "    â””â”€ The table lists individuals who disappeared and were later found, including their ages and dates.\n",
      "\n",
      "    ... and 27 more tables\n",
      "\n",
      "======================================================================\n",
      "âœ“ Table summary generation complete\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Define table info model (if not already imported)\n",
    "class TableInfo(BaseModel):\n",
    "    \"\"\"Information regarding a structured table.\"\"\"\n",
    "    table_name: str = Field(\n",
    "        ..., description=\"table name (must be underscores and NO spaces)\"\n",
    "    )\n",
    "    table_summary: str = Field(\n",
    "        ..., description=\"short, concise summary/caption of the table\"\n",
    "    )\n",
    "\n",
    "# Generate table summaries using LLM\n",
    "def generate_table_summary(df: pd.DataFrame, table_name: str) -> str:\n",
    "    \"\"\"Generate a concise summary for a table using LLM.\"\"\"\n",
    "    # Get sample rows (first 3)\n",
    "    sample_rows = df.head(3).to_string(index=False, max_cols=10)\n",
    "    columns = ', '.join(df.columns.tolist()[:10])  # First 10 columns\n",
    "    \n",
    "    if len(df.columns) > 10:\n",
    "        columns += f\" ... (+{len(df.columns) - 10} more)\"\n",
    "    \n",
    "    prompt = f\"\"\"Given a table with the following structure, provide a SHORT (one sentence, max 15 words) summary:\n",
    "\n",
    "Table: {table_name}\n",
    "Columns: {columns}\n",
    "Sample rows:\n",
    "{sample_rows}\n",
    "\n",
    "Provide ONLY a concise one-sentence summary of what this table contains.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = llm.complete(prompt)\n",
    "        return response.text.strip()\n",
    "    except Exception as e:\n",
    "        # Fallback if LLM fails\n",
    "        return f\"Table with {len(df.columns)} columns and {len(df)} rows\"\n",
    "\n",
    "# Generate table info for all tables\n",
    "print(f\"ðŸ¤– Generating table summaries using LLM...\")\n",
    "print(f\"   This makes {len(dfs)} API calls (LLM completions)\")\n",
    "print(f\"   Estimated time: {len(dfs) // 10} - {len(dfs) // 5} minutes\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "table_infos = []\n",
    "generation_errors = []\n",
    "\n",
    "for i, (df, filename) in enumerate(zip(dfs, table_filenames)):\n",
    "    # Clean table name (remove special characters, replace with underscores)\n",
    "    table_name = re.sub(r'\\W+', '_', filename).lower()\n",
    "    \n",
    "    # Ensure it doesn't start with a number (invalid SQL)\n",
    "    if table_name[0].isdigit():\n",
    "        table_name = 't_' + table_name\n",
    "    \n",
    "    try:\n",
    "        # Generate summary\n",
    "        summary = generate_table_summary(df, table_name)\n",
    "        \n",
    "        table_infos.append(\n",
    "            TableInfo(table_name=table_name, table_summary=summary)\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Fallback summary if generation fails\n",
    "        summary = f\"Wikipedia table with {len(df.columns)} columns\"\n",
    "        table_infos.append(\n",
    "            TableInfo(table_name=table_name, table_summary=summary)\n",
    "        )\n",
    "        generation_errors.append((table_name, str(e)))\n",
    "    \n",
    "    # Progress indicator every 10 tables\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"   âœ“ Generated summaries for {i + 1}/{len(dfs)} tables...\")\n",
    "\n",
    "print(f\"\\nâœ… Generated {len(table_infos)} table summaries\")\n",
    "\n",
    "if generation_errors:\n",
    "    print(f\"âš ï¸  {len(generation_errors)} summaries used fallback due to errors\")\n",
    "\n",
    "# Show first 10 examples\n",
    "print(f\"\\nðŸ“‹ Sample table summaries:\")\n",
    "print(\"=\"*70)\n",
    "for i, info in enumerate(table_infos[:10]):\n",
    "    print(f\"{i+1:2d}. {info.table_name}\")\n",
    "    print(f\"    â””â”€ {info.table_summary}\")\n",
    "\n",
    "if len(table_infos) > 10:\n",
    "    print(f\"\\n    ... and {len(table_infos) - 10} more tables\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"âœ“ Table summary generation complete\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Database Setup (FIXED - Scope Issue)\n",
    "\n",
    "**Previous Issue:** Engine variable was not properly scoped\n",
    "**Fix:** Properly define engine at module level\n",
    "\n",
    "### 3.1 Create SQLite Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Database engine created: wiki_full_dataset.db\n",
      "  Dialect: sqlite\n",
      "  Tables to create: 37\n"
     ]
    }
   ],
   "source": [
    "# Create database engine with proper scope\n",
    "DB_PATH = \"wiki_full_dataset.db\"\n",
    "engine = create_engine(f\"sqlite:///{DB_PATH}\")\n",
    "metadata_obj = MetaData()\n",
    "\n",
    "print(f\"âœ“ Database engine created: {DB_PATH}\")\n",
    "print(f\"  Dialect: {engine.dialect.name}\")\n",
    "print(f\"  Tables to create: {len(table_infos)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Create Tables and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ—„ï¸  Creating 37 database tables...\n",
      "   This will insert 1,128 total rows\n",
      "   Estimated time: 3 - 7 minutes\n",
      "======================================================================\n",
      "   âœ“ Created 25/37 tables...\n",
      "\n",
      "âœ… Database creation complete\n",
      "   Successfully created: 37 tables\n",
      "   Database size: 0.2 MB\n",
      "\n",
      "======================================================================\n",
      "âœ“ All tables created in wiki_full_dataset.db\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def sanitize_column_name(col_name: str) -> str:\n",
    "    \"\"\"Remove special characters and replace spaces with underscores.\"\"\"\n",
    "    return re.sub(r\"\\W+\", \"_\", col_name)\n",
    "\n",
    "def create_table_from_dataframe(\n",
    "    df: pd.DataFrame, table_name: str, engine, metadata_obj\n",
    "):\n",
    "    \"\"\"Create a table from a DataFrame using SQLAlchemy.\"\"\"\n",
    "    # Sanitize column names\n",
    "    sanitized_columns = {col: sanitize_column_name(col) for col in df.columns}\n",
    "    df = df.rename(columns=sanitized_columns)\n",
    "\n",
    "    # Dynamically create columns based on DataFrame\n",
    "    columns = [\n",
    "        Column(col, String if dtype == \"object\" else Integer)\n",
    "        for col, dtype in zip(df.columns, df.dtypes)\n",
    "    ]\n",
    "\n",
    "    # Create table\n",
    "    table = Table(table_name, metadata_obj, *columns)\n",
    "    metadata_obj.create_all(engine)\n",
    "\n",
    "    # Insert data\n",
    "    with engine.connect() as conn:\n",
    "        for _, row in df.iterrows():\n",
    "            insert_stmt = table.insert().values(**row.to_dict())\n",
    "            conn.execute(insert_stmt)\n",
    "        conn.commit()\n",
    "\n",
    "# Create tables for each DataFrame\n",
    "print(f\"ðŸ—„ï¸  Creating {len(dfs)} database tables...\")\n",
    "print(f\"   This will insert {sum(len(df) for df in dfs):,} total rows\")\n",
    "print(f\"   Estimated time: {len(dfs) // 10} - {len(dfs) // 5} minutes\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "creation_errors = []\n",
    "\n",
    "for idx, df in enumerate(dfs):\n",
    "    table_name = table_infos[idx].table_name\n",
    "    \n",
    "    try:\n",
    "        create_table_from_dataframe(df, table_name, engine, metadata_obj)\n",
    "        \n",
    "        # Progress indicator every 25 tables\n",
    "        if (idx + 1) % 25 == 0:\n",
    "            print(f\"   âœ“ Created {idx + 1}/{len(dfs)} tables...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        creation_errors.append((table_name, str(e)))\n",
    "\n",
    "print(f\"\\nâœ… Database creation complete\")\n",
    "print(f\"   Successfully created: {len(dfs) - len(creation_errors)} tables\")\n",
    "\n",
    "if creation_errors:\n",
    "    print(f\"   âš ï¸  Failed to create: {len(creation_errors)} tables\")\n",
    "    for table, error in creation_errors[:3]:\n",
    "        print(f\"      - {table}: {error}\")\n",
    "\n",
    "# Get database size\n",
    "db_size_mb = Path(DB_PATH).stat().st_size / (1024*1024)\n",
    "print(f\"   Database size: {db_size_mb:.1f} MB\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"âœ“ All tables created in {DB_PATH}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Verify Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database tables:\n",
      "\n",
      "t_0:\n",
      "Table 't_0' has columns: Year (INTEGER), Title (VARCHAR), Chart_Positions_UK (VARCHAR), Chart_Positions_US (VARCHAR), Chart_Positions_NL (VARCHAR), Comments (VARCHAR), .\n",
      "\n",
      "t_1:\n",
      "Table 't_1' has columns: Year (INTEGER), Title (VARCHAR), Role (VARCHAR), Notes (VARCHAR), .\n",
      "\n",
      "t_10:\n",
      "Table 't_10' has columns: year (INTEGER), deaths (VARCHAR), _of_accidents (INTEGER), .\n",
      "\n",
      "t_11:\n",
      "Table 't_11' has columns: Award (VARCHAR), Category (VARCHAR), Nominee (VARCHAR), Result (VARCHAR), .\n",
      "\n",
      "t_12:\n",
      "Table 't_12' has columns: Year (VARCHAR), Award (VARCHAR), Category (VARCHAR), Nominated_work (VARCHAR), Result (VARCHAR), .\n",
      "\n",
      "t_14:\n",
      "Table 't_14' has columns: Act (VARCHAR), Year_signed (INTEGER), _Albums_released_under_Bad_Boy (VARCHAR), .\n",
      "\n",
      "t_15:\n",
      "Table 't_15' has columns: Year (VARCHAR), Title (VARCHAR), Role (VARCHAR), Notes (VARCHAR), .\n",
      "\n",
      "t_17:\n",
      "Table 't_17' has columns: Year (INTEGER), Single (VARCHAR), Peak_chart_positions_US (VARCHAR), Peak_chart_positions_US_R_B (VARCHAR), Peak_chart_positions_US_A_C (VARCHAR), Peak_chart_positions_UK (VARCHAR), .\n",
      "\n",
      "t_18:\n",
      "Table 't_18' has columns: Frequency (VARCHAR), Call_sign (VARCHAR), Name (VARCHAR), Format (VARCHAR), Owner (VARCHAR), Target_city_market (VARCHAR), City_of_license (VARCHAR), .\n",
      "\n",
      "t_20:\n",
      "Table 't_20' has columns: _ (INTEGER), Name (VARCHAR), Age (VARCHAR), Disappeared (VARCHAR), Found (VARCHAR), .\n",
      "\n",
      "t_22:\n",
      "Table 't_22' has columns: Year (INTEGER), Single (VARCHAR), Peak_chart_positions_GER (VARCHAR), Peak_chart_positions_IRE (VARCHAR), Peak_chart_positions_UK (VARCHAR), Peak_chart_positions_US (VARCHAR), Peak_chart_positions_US_Main (VARCHAR), Peak_chart_positions_US_Dance (VARCHAR), Certifications_sales_thresholds_ (INTEGER), Album (VARCHAR), .\n",
      "\n",
      "t_24:\n",
      "Table 't_24' has columns: Film (VARCHAR), Film_1 (VARCHAR), Date (VARCHAR), .\n",
      "\n",
      "t_25:\n",
      "Table 't_25' has columns: Service (VARCHAR), 2012_13_Total_Cost_million_ (INTEGER), Comparison_with_2011_12_million_ (VARCHAR), .\n",
      "\n",
      "t_26:\n",
      "Table 't_26' has columns: City_served_Location (VARCHAR), ICAO (VARCHAR), IATA (VARCHAR), Airport_name (VARCHAR), Usage (VARCHAR), .\n",
      "\n",
      "t_28:\n",
      "Table 't_28' has columns: Party (VARCHAR), Active_voters (VARCHAR), Inactive_voters (VARCHAR), Total_voters (VARCHAR), Percentage (VARCHAR), .\n",
      "\n",
      "t_29:\n",
      "Table 't_29' has columns: Club_performance_Season_Norway (VARCHAR), Club_performance_Club_Norway (VARCHAR), Club_performance_League_Norway (VARCHAR), League_Apps_League (VARCHAR), League_Goals_League (VARCHAR), Cup_Apps_Norwegian_Cup (VARCHAR), Cup_Goals_Norwegian_Cup (VARCHAR), Total_Apps_Total (VARCHAR), Total_Goals_Total (VARCHAR), .\n",
      "\n",
      "t_3:\n",
      "Table 't_3' has columns: Year (INTEGER), Winner (VARCHAR), Jockey (VARCHAR), Trainer (VARCHAR), Owner (VARCHAR), Breeder (VARCHAR), .\n",
      "\n",
      "t_30:\n",
      "Table 't_30' has columns: Year (INTEGER), Award (VARCHAR), Work_Artist (VARCHAR), Result (VARCHAR), .\n",
      "\n",
      "t_31:\n",
      "Table 't_31' has columns: Result (VARCHAR), Record (VARCHAR), Opponent (VARCHAR), Method (VARCHAR), Date (VARCHAR), Round (INTEGER), Time (VARCHAR), Event (VARCHAR), Location (VARCHAR), Notes (VARCHAR), .\n",
      "\n",
      "t_32:\n",
      "Table 't_32' has columns: Year (INTEGER), W (VARCHAR), L (VARCHAR), T (VARCHAR), Finish (VARCHAR), Coach (VARCHAR), .\n",
      "\n",
      "t_33:\n",
      "Table 't_33' has columns: District (VARCHAR), Area_Size_kmÂ²_ (VARCHAR), Population (VARCHAR), Density_per_kmÂ² (VARCHAR), .\n",
      "\n",
      "t_34:\n",
      "Table 't_34' has columns: Character (VARCHAR), Actor (VARCHAR), Series (VARCHAR), Notes (VARCHAR), .\n",
      "\n",
      "t_35:\n",
      "Table 't_35' has columns: Party (VARCHAR), Active_Voters (VARCHAR), Inactive_Voters (INTEGER), Total_Voters (VARCHAR), Percentage (VARCHAR), .\n",
      "\n",
      "t_36:\n",
      "Table 't_36' has columns: Year (INTEGER), Award (VARCHAR), Film (VARCHAR), Result (VARCHAR), .\n",
      "\n",
      "t_37:\n",
      "Table 't_37' has columns: Preceded_by_Alfred_Scott (VARCHAR), Member_of_Parliament_for_Ashton_under_Lyne_1910_1916 (VARCHAR), Succeeded_by_Albert_Stanley (VARCHAR), .\n",
      "\n",
      "t_38:\n",
      "Table 't_38' has columns: New_municipality (VARCHAR), Old_municipalities (VARCHAR), Seat (VARCHAR), .\n",
      "\n",
      "t_4:\n",
      "Table 't_4' has columns: Team (VARCHAR), P (INTEGER), W (INTEGER), T (INTEGER), L (INTEGER), GF (INTEGER), GA (INTEGER), GD (VARCHAR), Pts_ (INTEGER), .\n",
      "\n",
      "t_41:\n",
      "Table 't_41' has columns: Number (VARCHAR), Encoding (INTEGER), Implied_probability (VARCHAR), .\n",
      "\n",
      "t_42:\n",
      "Table 't_42' has columns: Month (VARCHAR), Jan (VARCHAR), Feb (VARCHAR), Mar (VARCHAR), Apr (VARCHAR), May (VARCHAR), Jun (VARCHAR), Jul (VARCHAR), Aug (VARCHAR), Sep (VARCHAR), Oct (VARCHAR), Nov (VARCHAR), Dec (VARCHAR), Year (VARCHAR), .\n",
      "\n",
      "t_44:\n",
      "Table 't_44' has columns: Name (VARCHAR), Term_start (VARCHAR), Term_end (VARCHAR), .\n",
      "\n",
      "t_45:\n",
      "Table 't_45' has columns: _ (INTEGER), Office (VARCHAR), Current_Officer (VARCHAR), .\n",
      "\n",
      "t_46:\n",
      "Table 't_46' has columns: Month (VARCHAR), Jan (VARCHAR), Feb (VARCHAR), Mar (VARCHAR), Apr (VARCHAR), May (VARCHAR), Jun (VARCHAR), Jul (VARCHAR), Aug (VARCHAR), Sep (VARCHAR), Oct (VARCHAR), Nov (VARCHAR), Dec (VARCHAR), Year (VARCHAR), .\n",
      "\n",
      "t_47:\n",
      "Table 't_47' has columns: Date (VARCHAR), Event (VARCHAR), Duration_months_ (VARCHAR), Duration_years_ (VARCHAR), .\n",
      "\n",
      "t_48:\n",
      "Table 't_48' has columns: Month (VARCHAR), Jan (VARCHAR), Feb (VARCHAR), Mar (VARCHAR), Apr (VARCHAR), May (VARCHAR), Jun (VARCHAR), Jul (VARCHAR), Aug (VARCHAR), Sep (VARCHAR), Oct (VARCHAR), Nov (VARCHAR), Dec (VARCHAR), Year (VARCHAR), .\n",
      "\n",
      "t_7:\n",
      "Table 't_7' has columns: Afrikaans (VARCHAR), IPA (VARCHAR), Dutch (VARCHAR), English (VARCHAR), German (VARCHAR), .\n",
      "\n",
      "t_8:\n",
      "Table 't_8' has columns: District (VARCHAR), Location (VARCHAR), Communities_served (VARCHAR), .\n",
      "\n",
      "t_9:\n",
      "Table 't_9' has columns: Category (VARCHAR), Examples (VARCHAR), Cancers (VARCHAR), Gene_functions (VARCHAR), .\n"
     ]
    }
   ],
   "source": [
    "# Create SQLDatabase object\n",
    "sql_database = SQLDatabase(engine)\n",
    "\n",
    "print(\"Database tables:\")\n",
    "for table_name in sql_database.get_usable_table_names():\n",
    "    print(f\"\\n{table_name}:\")\n",
    "    print(sql_database.get_single_table_info(table_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Workflow 1 - Query-Time Table Retrieval\n",
    "\n",
    "### 4.1 Setup ObjectIndex for Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Building ObjectIndex with 37 table schemas...\n",
      "   This creates embeddings for table summaries (1 API call)\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… ObjectIndex created successfully\n",
      "   Indexed 37 table schemas\n",
      "\n",
      "âœ“ Table retriever configured\n",
      "  Will retrieve top 2 most relevant tables per query\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create table node mapping\n",
    "table_node_mapping = SQLTableNodeMapping(sql_database)\n",
    "\n",
    "# Create table schema objects with context\n",
    "table_schema_objs = [\n",
    "    SQLTableSchema(table_name=t.table_name, context_str=t.table_summary)\n",
    "    for t in table_infos\n",
    "]\n",
    "\n",
    "print(f\"ðŸ” Building ObjectIndex with {len(table_schema_objs)} table schemas...\")\n",
    "print(f\"   This creates embeddings for table summaries (1 API call)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Build ObjectIndex (this makes embedding API calls)\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    table_schema_objs,\n",
    "    table_node_mapping,\n",
    "    VectorStoreIndex,\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… ObjectIndex created successfully\")\n",
    "print(f\"   Indexed {len(table_schema_objs)} table schemas\")\n",
    "\n",
    "# Create retriever\n",
    "obj_retriever = obj_index.as_retriever(similarity_top_k=2)\n",
    "\n",
    "print(f\"\\nâœ“ Table retriever configured\")\n",
    "print(f\"  Will retrieve top 2 most relevant tables per query\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Setup SQL Retriever and Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ SQL retriever and helper functions configured\n"
     ]
    }
   ],
   "source": [
    "# Create SQL retriever\n",
    "sql_retriever = SQLRetriever(sql_database)\n",
    "\n",
    "# Helper function to get table context\n",
    "def get_table_context_str(table_schema_objs: List[SQLTableSchema]) -> str:\n",
    "    \"\"\"Get table context string.\"\"\"\n",
    "    context_strs = []\n",
    "    for table_schema_obj in table_schema_objs:\n",
    "        table_info = sql_database.get_single_table_info(table_schema_obj.table_name)\n",
    "        if table_schema_obj.context_str:\n",
    "            table_opt_context = \" The table description is: \"\n",
    "            table_opt_context += table_schema_obj.context_str\n",
    "            table_info += table_opt_context\n",
    "        context_strs.append(table_info)\n",
    "    return \"\\n\\n\".join(context_strs)\n",
    "\n",
    "# SQL parsing function\n",
    "def parse_response_to_sql(chat_response: ChatResponse) -> str:\n",
    "    \"\"\"Parse response to SQL.\"\"\"\n",
    "    response = chat_response.message.content\n",
    "    sql_query_start = response.find(\"SQLQuery:\")\n",
    "    if sql_query_start != -1:\n",
    "        response = response[sql_query_start:]\n",
    "        if response.startswith(\"SQLQuery:\"):\n",
    "            response = response[len(\"SQLQuery:\"):]\n",
    "    sql_result_start = response.find(\"SQLResult:\")\n",
    "    if sql_result_start != -1:\n",
    "        response = response[:sql_result_start]\n",
    "    return response.strip().strip(\"```\").strip()\n",
    "\n",
    "print(\"âœ“ SQL retriever and helper functions configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Create Text-to-SQL Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Prompts configured\n"
     ]
    }
   ],
   "source": [
    "# Text-to-SQL prompt\n",
    "text2sql_prompt_str = f\"\"\"\n",
    "Given an input question, first create a syntactically correct {engine.dialect.name} query to run, \n",
    "then look at the results of the query and return the answer. \n",
    "\n",
    "Pay attention to use only the column names that you can see in the schema description. \n",
    "Be careful to not query for columns that do not exist. \n",
    "Qualify column names with the table name when needed.\n",
    "\n",
    "You are required to use the following format:\n",
    "\n",
    "Question: Question here\n",
    "SQLQuery: SQL Query to run\n",
    "SQLResult: Result of the SQLQuery\n",
    "Answer: Final answer here\n",
    "\n",
    "Only use tables listed below.\n",
    "{{schema}}\n",
    "\n",
    "Question: {{query_str}}\n",
    "SQLQuery: \n",
    "\"\"\"\n",
    "\n",
    "text2sql_prompt = PromptTemplate(text2sql_prompt_str)\n",
    "\n",
    "# Response synthesis prompt\n",
    "response_synthesis_prompt_str = (\n",
    "    \"Given an input question, synthesize a response from the query results.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"SQL: {sql_query}\\n\"\n",
    "    \"SQL Response: {context_str}\\n\"\n",
    "    \"Response: \"\n",
    ")\n",
    "response_synthesis_prompt = PromptTemplate(response_synthesis_prompt_str)\n",
    "\n",
    "print(\"âœ“ Prompts configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Define Workflow Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Workflow events defined\n"
     ]
    }
   ],
   "source": [
    "class TableRetrieveEvent(Event):\n",
    "    \"\"\"Result of running table retrieval.\"\"\"\n",
    "    table_context_str: str\n",
    "    query: str\n",
    "\n",
    "class TextToSQLEvent(Event):\n",
    "    \"\"\"Text-to-SQL event.\"\"\"\n",
    "    sql: str\n",
    "    query: str\n",
    "\n",
    "print(\"âœ“ Workflow events defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Define TextToSQLWorkflow1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ TextToSQLWorkflow1 defined\n"
     ]
    }
   ],
   "source": [
    "class TextToSQLWorkflow1(Workflow):\n",
    "    \"\"\"Text-to-SQL Workflow with query-time table retrieval.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        obj_retriever,\n",
    "        text2sql_prompt,\n",
    "        sql_retriever,\n",
    "        response_synthesis_prompt,\n",
    "        llm,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.obj_retriever = obj_retriever\n",
    "        self.text2sql_prompt = text2sql_prompt\n",
    "        self.sql_retriever = sql_retriever\n",
    "        self.response_synthesis_prompt = response_synthesis_prompt\n",
    "        self.llm = llm\n",
    "\n",
    "    @step\n",
    "    def retrieve_tables(\n",
    "        self, ctx: Context, ev: StartEvent\n",
    "    ) -> TableRetrieveEvent:\n",
    "        \"\"\"Retrieve relevant tables.\"\"\"\n",
    "        table_schema_objs = self.obj_retriever.retrieve(ev.query)\n",
    "        table_context_str = get_table_context_str(table_schema_objs)\n",
    "        return TableRetrieveEvent(\n",
    "            table_context_str=table_context_str, query=ev.query\n",
    "        )\n",
    "\n",
    "    @step\n",
    "    def generate_sql(\n",
    "        self, ctx: Context, ev: TableRetrieveEvent\n",
    "    ) -> TextToSQLEvent:\n",
    "        \"\"\"Generate SQL statement.\"\"\"\n",
    "        fmt_messages = self.text2sql_prompt.format_messages(\n",
    "            query_str=ev.query, schema=ev.table_context_str\n",
    "        )\n",
    "        chat_response = self.llm.chat(fmt_messages)\n",
    "        sql = parse_response_to_sql(chat_response)\n",
    "        return TextToSQLEvent(sql=sql, query=ev.query)\n",
    "\n",
    "    @step\n",
    "    def generate_response(self, ctx: Context, ev: TextToSQLEvent) -> StopEvent:\n",
    "        \"\"\"Run SQL retrieval and generate response.\"\"\"\n",
    "        retrieved_rows = self.sql_retriever.retrieve(ev.sql)\n",
    "        fmt_messages = self.response_synthesis_prompt.format_messages(\n",
    "            sql_query=ev.sql,\n",
    "            context_str=str(retrieved_rows),\n",
    "            query_str=ev.query,\n",
    "        )\n",
    "        chat_response = self.llm.chat(fmt_messages)\n",
    "        return StopEvent(result=chat_response)\n",
    "\n",
    "print(\"âœ“ TextToSQLWorkflow1 defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Run Workflow 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Workflow1 initialized\n",
      "  Ready to process queries with table retrieval\n"
     ]
    }
   ],
   "source": [
    "# Create workflow instance\n",
    "workflow1 = TextToSQLWorkflow1(\n",
    "    obj_retriever,\n",
    "    text2sql_prompt,\n",
    "    sql_retriever,\n",
    "    response_synthesis_prompt,\n",
    "    llm,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"âœ“ Workflow1 initialized\")\n",
    "print(\"  Ready to process queries with table retrieval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What was the year that The Notorious B.I.G was signed?\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: assistant: The Notorious B.I.G was signed in the year 1993.\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test query 1\n",
    "import asyncio\n",
    "\n",
    "query = \"What was the year that The Notorious B.I.G was signed?\"\n",
    "print(f\"Query: {query}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "response = await workflow1.run(query=query)\n",
    "print(f\"\\nAnswer: {response}\")\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Who won best director in the 1972 academy awards?\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: assistant: It appears that there were no results found for the Best Director category in the 1972 Academy Awards. Therefore, I cannot provide the name of the winner.\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test query 2\n",
    "query = \"Who won best director in the 1972 academy awards?\"\n",
    "print(f\"Query: {query}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "response = await workflow1.run(query=query)\n",
    "print(f\"\\nAnswer: {response}\")\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Which city has the highest population?\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: assistant: The city with the highest population is GojÅ, Yoshino, which has a population of 92.\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test query 3\n",
    "query = \"Which city has the highest population?\"\n",
    "print(f\"Query: {query}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "response = await workflow1.run(query=query)\n",
    "print(f\"\\nAnswer: {response}\")\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Workflow 2 - Table + Row Retrieval\n",
    "\n",
    "### 5.1 Index All Table Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš¡ Starting row indexing process...\n",
      "   This is the most time-consuming step for large datasets\n",
      "\n",
      "ðŸ“Š Indexing rows from 37 tables...\n",
      "   Max rows per table: 100\n",
      "   This makes embedding API calls for each table's rows\n",
      "   Estimated time: 3 - 7 minutes\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ“ Indexed 10/37 tables (217 rows so far)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ“ Indexed 20/37 tables (472 rows so far)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ“ Indexed 30/37 tables (602 rows so far)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Row indexing complete\n",
      "   Successfully indexed: 37 tables\n",
      "   Total rows indexed: 666\n",
      "======================================================================\n",
      "\n",
      "âœ“ Row retrieval system ready\n",
      "  Indexed 37 tables\n",
      "  Can retrieve relevant example rows to improve SQL generation\n"
     ]
    }
   ],
   "source": [
    "# Configuration for row indexing\n",
    "MAX_ROWS_PER_TABLE = 100  # Limit rows per table to prevent memory issues\n",
    "\n",
    "def index_all_tables(\n",
    "    sql_database: SQLDatabase, \n",
    "    table_index_dir: str = \"table_index_dir\",\n",
    "    max_rows_per_table: int = MAX_ROWS_PER_TABLE\n",
    ") -> Dict[str, VectorStoreIndex]:\n",
    "    \"\"\"Index all table rows for retrieval with row limits.\"\"\"\n",
    "    if not Path(table_index_dir).exists():\n",
    "        os.makedirs(table_index_dir)\n",
    "\n",
    "    vector_index_dict = {}\n",
    "    engine = sql_database.engine\n",
    "    all_tables = sql_database.get_usable_table_names()\n",
    "    \n",
    "    print(f\"ðŸ“Š Indexing rows from {len(all_tables)} tables...\")\n",
    "    print(f\"   Max rows per table: {max_rows_per_table}\")\n",
    "    print(f\"   This makes embedding API calls for each table's rows\")\n",
    "    print(f\"   Estimated time: {len(all_tables) // 10} - {len(all_tables) // 5} minutes\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    indexing_errors = []\n",
    "    total_rows_indexed = 0\n",
    "    \n",
    "    for idx, table_name in enumerate(all_tables):\n",
    "        table_path = Path(table_index_dir) / table_name\n",
    "        \n",
    "        try:\n",
    "            if not table_path.exists():\n",
    "                # Get rows from table (with limit)\n",
    "                with engine.connect() as conn:\n",
    "                    # Use LIMIT to prevent memory issues\n",
    "                    query = f'SELECT * FROM \"{table_name}\" LIMIT {max_rows_per_table}'\n",
    "                    result = conn.execute(text(query))\n",
    "                    rows = result.fetchall()\n",
    "                    row_tups = [tuple(row) for row in rows]\n",
    "                \n",
    "                if len(row_tups) == 0:\n",
    "                    # Skip empty tables\n",
    "                    continue\n",
    "\n",
    "                # Create text nodes from rows\n",
    "                nodes = [TextNode(text=str(t)) for t in row_tups]\n",
    "\n",
    "                # Create vector index (this makes embedding API call)\n",
    "                index = VectorStoreIndex(nodes)\n",
    "                index.set_index_id(\"vector_index\")\n",
    "                index.storage_context.persist(str(table_path))\n",
    "                \n",
    "                total_rows_indexed += len(row_tups)\n",
    "                \n",
    "            else:\n",
    "                # Load existing index\n",
    "                storage_context = StorageContext.from_defaults(persist_dir=str(table_path))\n",
    "                index = load_index_from_storage(storage_context, index_id=\"vector_index\")\n",
    "\n",
    "            vector_index_dict[table_name] = index\n",
    "            \n",
    "            # Progress indicator every 10 tables\n",
    "            if (idx + 1) % 10 == 0:\n",
    "                print(f\"   âœ“ Indexed {idx + 1}/{len(all_tables)} tables ({total_rows_indexed} rows so far)...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            indexing_errors.append((table_name, str(e)))\n",
    "\n",
    "    print(f\"\\nâœ… Row indexing complete\")\n",
    "    print(f\"   Successfully indexed: {len(vector_index_dict)} tables\")\n",
    "    print(f\"   Total rows indexed: {total_rows_indexed}\")\n",
    "    \n",
    "    if indexing_errors:\n",
    "        print(f\"   âš ï¸  Failed to index: {len(indexing_errors)} tables\")\n",
    "        for table, error in indexing_errors[:3]:\n",
    "            print(f\"      - {table}: {error}\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return vector_index_dict\n",
    "\n",
    "# Index all tables with row limits\n",
    "print(\"\\nâš¡ Starting row indexing process...\")\n",
    "print(\"   This is the most time-consuming step for large datasets\")\n",
    "print()\n",
    "\n",
    "vector_index_dict = index_all_tables(sql_database, max_rows_per_table=MAX_ROWS_PER_TABLE)\n",
    "\n",
    "print(f\"\\nâœ“ Row retrieval system ready\")\n",
    "print(f\"  Indexed {len(vector_index_dict)} tables\")\n",
    "print(f\"  Can retrieve relevant example rows to improve SQL generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Enhanced Table Context with Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Enhanced context function defined\n"
     ]
    }
   ],
   "source": [
    "def get_table_context_and_rows_str(\n",
    "    query_str: str,\n",
    "    table_schema_objs: List[SQLTableSchema],\n",
    "    verbose: bool = False,\n",
    ") -> str:\n",
    "    \"\"\"Get table context string with example rows.\"\"\"\n",
    "    context_strs = []\n",
    "    \n",
    "    for table_schema_obj in table_schema_objs:\n",
    "        # Get table info\n",
    "        table_info = sql_database.get_single_table_info(table_schema_obj.table_name)\n",
    "        \n",
    "        if table_schema_obj.context_str:\n",
    "            table_opt_context = \" The table description is: \"\n",
    "            table_opt_context += table_schema_obj.context_str\n",
    "            table_info += table_opt_context\n",
    "\n",
    "        # Retrieve relevant rows\n",
    "        vector_retriever = vector_index_dict[table_schema_obj.table_name].as_retriever(\n",
    "            similarity_top_k=2\n",
    "        )\n",
    "        relevant_nodes = vector_retriever.retrieve(query_str)\n",
    "        \n",
    "        if len(relevant_nodes) > 0:\n",
    "            table_row_context = (\n",
    "                \"\\nHere are some relevant example rows (values in the same order as columns above)\\n\"\n",
    "            )\n",
    "            for node in relevant_nodes:\n",
    "                table_row_context += str(node.get_content()) + \"\\n\"\n",
    "            table_info += table_row_context\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"> Table Info: {table_info}\")\n",
    "\n",
    "        context_strs.append(table_info)\n",
    "    \n",
    "    return \"\\n\\n\".join(context_strs)\n",
    "\n",
    "print(\"âœ“ Enhanced context function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Define TextToSQLWorkflow2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ TextToSQLWorkflow2 defined\n"
     ]
    }
   ],
   "source": [
    "class TextToSQLWorkflow2(TextToSQLWorkflow1):\n",
    "    \"\"\"Text-to-SQL Workflow with table AND row retrieval.\"\"\"\n",
    "\n",
    "    @step\n",
    "    def retrieve_tables(\n",
    "        self, ctx: Context, ev: StartEvent\n",
    "    ) -> TableRetrieveEvent:\n",
    "        \"\"\"Retrieve tables with example rows.\"\"\"\n",
    "        table_schema_objs = self.obj_retriever.retrieve(ev.query)\n",
    "        table_context_str = get_table_context_and_rows_str(\n",
    "            ev.query, table_schema_objs, verbose=self._verbose\n",
    "        )\n",
    "        return TableRetrieveEvent(\n",
    "            table_context_str=table_context_str, query=ev.query\n",
    "        )\n",
    "\n",
    "print(\"âœ“ TextToSQLWorkflow2 defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Run Workflow 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Workflow2 initialized\n",
      "  Ready to process queries with table AND row retrieval\n"
     ]
    }
   ],
   "source": [
    "# Create workflow instance\n",
    "workflow2 = TextToSQLWorkflow2(\n",
    "    obj_retriever,\n",
    "    text2sql_prompt,\n",
    "    sql_retriever,\n",
    "    response_synthesis_prompt,\n",
    "    llm,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"âœ“ Workflow2 initialized\")\n",
    "print(\"  Ready to process queries with table AND row retrieval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What was the year that The Notorious BIG was signed to Bad Boy?\n",
      "======================================================================\n",
      "Note: Different spelling 'BIG' vs 'B.I.G' - row retrieval helps!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Table Info: Table 't_14' has columns: Act (VARCHAR), Year_signed (INTEGER), _Albums_released_under_Bad_Boy (VARCHAR), . The table description is: The table lists acts signed to Bad Boy, their signing year, and album releases.\n",
      "Here are some relevant example rows (values in the same order as columns above)\n",
      "('The Notorious B.I.G', 1993, '5')\n",
      "('Diddy', 1993, '6')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Table Info: Table 't_32' has columns: Year (INTEGER), W (VARCHAR), L (VARCHAR), T (VARCHAR), Finish (VARCHAR), Coach (VARCHAR), . The table description is: The table records annual football team performance statistics under Coach Earl Ball from 1910 to 1912.\n",
      "Here are some relevant example rows (values in the same order as columns above)\n",
      "(1913, '8', '2', '0', 'Indiana State Champs', 'Earl Ball')\n",
      "(1921, '0', '2', '0', '18th APFA', 'Cooney Checkaye')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: assistant: The Notorious B.I.G. was signed to Bad Boy in 1993.\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test with row retrieval\n",
    "query = \"What was the year that The Notorious BIG was signed to Bad Boy?\"\n",
    "print(f\"Query: {query}\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Note: Different spelling 'BIG' vs 'B.I.G' - row retrieval helps!\\n\")\n",
    "\n",
    "response = await workflow2.run(query=query)\n",
    "print(f\"\\nAnswer: {response}\")\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Production Considerations (NEW)\n",
    "\n",
    "### 6.1 SQL Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:Dangerous keyword detected: DROP\n",
      "WARNING:__main__:Dangerous keyword detected: DELETE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL Validation Tests:\n",
      "SELECT * FROM users: True\n",
      "DROP TABLE users: False\n",
      "DELETE FROM users: False\n"
     ]
    }
   ],
   "source": [
    "def validate_sql(sql: str, verbose: bool = True) -> bool:\n",
    "    \"\"\"Validate SQL query for safety.\"\"\"\n",
    "    dangerous_keywords = [\n",
    "        'DROP', 'DELETE', 'UPDATE', 'INSERT', 'ALTER', \n",
    "        'TRUNCATE', 'EXEC', 'EXECUTE', 'CREATE'\n",
    "    ]\n",
    "    \n",
    "    sql_upper = sql.upper()\n",
    "    \n",
    "    for keyword in dangerous_keywords:\n",
    "        if keyword in sql_upper:\n",
    "            if verbose:\n",
    "                logger.warning(f\"Dangerous keyword detected: {keyword}\")\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Test validation\n",
    "print(\"SQL Validation Tests:\")\n",
    "print(f\"SELECT * FROM users: {validate_sql('SELECT * FROM users')}\")\n",
    "print(f\"DROP TABLE users: {validate_sql('DROP TABLE users')}\")\n",
    "print(f\"DELETE FROM users: {validate_sql('DELETE FROM users')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Production Workflow with Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Production workflow defined with error handling\n"
     ]
    }
   ],
   "source": [
    "class ProductionTextToSQLWorkflow(TextToSQLWorkflow2):\n",
    "    \"\"\"Production-ready workflow with error handling and validation.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, max_retries: int = 3, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.max_retries = max_retries\n",
    "\n",
    "    @step\n",
    "    def generate_sql(\n",
    "        self, ctx: Context, ev: TableRetrieveEvent\n",
    "    ) -> TextToSQLEvent:\n",
    "        \"\"\"Generate SQL with validation and retry logic.\"\"\"\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                fmt_messages = self.text2sql_prompt.format_messages(\n",
    "                    query_str=ev.query, schema=ev.table_context_str\n",
    "                )\n",
    "                chat_response = self.llm.chat(fmt_messages)\n",
    "                sql = parse_response_to_sql(chat_response)\n",
    "\n",
    "                # Validate SQL\n",
    "                if validate_sql(sql, verbose=True):\n",
    "                    logger.info(f\"Generated valid SQL: {sql}\")\n",
    "                    return TextToSQLEvent(sql=sql, query=ev.query)\n",
    "                else:\n",
    "                    logger.warning(\n",
    "                        f\"Invalid SQL generated (attempt {attempt + 1}/{self.max_retries})\"\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                logger.error(\n",
    "                    f\"Error generating SQL (attempt {attempt + 1}/{self.max_retries}): {e}\"\n",
    "                )\n",
    "\n",
    "        raise ValueError(f\"Failed to generate valid SQL after {self.max_retries} attempts\")\n",
    "\n",
    "print(\"âœ“ Production workflow defined with error handling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Production Checklist\n",
    "\n",
    "Before deploying to production:\n",
    "\n",
    "**Security:**\n",
    "- [ ] Use read-only database connections\n",
    "- [ ] Implement query validation\n",
    "- [ ] Set query timeouts\n",
    "- [ ] Use database roles with minimal privileges\n",
    "- [ ] Monitor and log all queries\n",
    "\n",
    "**Performance:**\n",
    "- [ ] Implement caching for common queries\n",
    "- [ ] Set appropriate similarity_top_k values\n",
    "- [ ] Use connection pooling\n",
    "- [ ] Monitor query execution times\n",
    "\n",
    "**Reliability:**\n",
    "- [ ] Implement retry logic\n",
    "- [ ] Add proper error handling\n",
    "- [ ] Set up monitoring and alerting\n",
    "- [ ] Test with production data volumes\n",
    "\n",
    "**Cost:**\n",
    "- [ ] Monitor LLM API usage\n",
    "- [ ] Implement rate limiting\n",
    "- [ ] Cache embeddings when possible\n",
    "- [ ] Use appropriate model sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "âœ“ LlamaIndex Workflows architecture\n",
    "âœ“ Query-time table retrieval workflows\n",
    "âœ“ Query-time row retrieval with vector indices\n",
    "âœ“ Production-ready patterns and error handling\n",
    "âœ“ SQL validation and security best practices\n",
    "âœ“ How to fix common issues (API key exposure, scope issues)\n",
    "\n",
    "## Key Improvements from Original\n",
    "\n",
    "1. **Security**: Replaced hardcoded API keys with environment variables\n",
    "2. **Scope**: Fixed engine variable scope issues\n",
    "3. **Portability**: Removed Google Colab-specific code\n",
    "4. **Production**: Added error handling and SQL validation\n",
    "5. **Documentation**: Enhanced explanations and best practices\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Explore **Notebook 04: PostgreSQL Integration** for production databases\n",
    "- Implement your own custom workflows\n",
    "- Add domain-specific table descriptions\n",
    "- Experiment with different embedding models\n",
    "- Test with your own datasets\n",
    "\n",
    "---\n",
    "\n",
    "**Outstanding work!** You've mastered advanced text-to-SQL techniques! ðŸŽ¯"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
