{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02: Intermediate Text-to-SQL with Dynamic Table Retrieval\n",
    "\n",
    "Welcome to the second notebook! In this notebook, you'll learn how to scale text-to-SQL to larger databases using dynamic table retrieval.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "- Work with multi-table databases\n",
    "- Use `SQLTableRetrieverQueryEngine` for large schemas\n",
    "- Implement dynamic table retrieval with ObjectIndex\n",
    "- Understand when to use different query engines\n",
    "- Query CSV files directly with DuckDB\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "In Notebook 01, we sent all table schemas to the LLM with every query. This works for small databases but doesn't scale to databases with many tables. In this notebook, you'll learn how to use **vector similarity** to dynamically retrieve only the most relevant tables for each query.\n",
    "\n",
    "## Dynamic Table Retrieval\n",
    "\n",
    "When you have many tables, sending all schemas to the LLM becomes impractical. Dynamic table retrieval uses vector similarity to find the most relevant tables for each query.\n",
    "\n",
    "**Problem:** Database has 50 tables, but LLM context limit is 100K tokens  \n",
    "**Solution:** Retrieve only the 2-3 most relevant tables based on the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup and Configuration\n",
    "\n",
    "### 1.1 Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "# Third-party imports\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, text\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# LlamaIndex imports\n",
    "from llama_index.core import SQLDatabase, VectorStoreIndex\n",
    "from llama_index.core.indices.struct_store import SQLTableRetrieverQueryEngine\n",
    "from llama_index.core.objects import (\n",
    "    SQLTableNodeMapping,\n",
    "    ObjectIndex,\n",
    "    SQLTableSchema,\n",
    ")\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load Environment and Initialize LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Environment loaded and LLM initialized\n",
      "  Model: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found. Please check your .env file\")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = OpenAI(\n",
    "    temperature=0.1,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "print(\"✓ Environment loaded and LLM initialized\")\n",
    "print(f\"  Model: {llm.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Create SQLite Database\n",
    "\n",
    "We'll use SQLite for the main tutorial because it has excellent compatibility with LlamaIndex's SQLDatabase class. In Section 5, we'll demonstrate DuckDB's powerful features for querying CSV files directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SQLite connection successful (test value: 1)\n",
      "  Database: ecommerce.db\n",
      "  Dialect: sqlite\n"
     ]
    }
   ],
   "source": [
    "# Create SQLite database (compatible with LlamaIndex SQLDatabase)\n",
    "# Note: We use SQLite instead of DuckDB for the main tutorial because SQLite\n",
    "# has better compatibility with SQLAlchemy's reflection system used by LlamaIndex.\n",
    "# We'll still demonstrate DuckDB's powerful features in Section 5!\n",
    "\n",
    "engine = create_engine(\"sqlite:///ecommerce.db\")\n",
    "\n",
    "# Test connection\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(\"SELECT 1 as test\"))\n",
    "    test_value = result.fetchone()[0]\n",
    "    print(f\"✓ SQLite connection successful (test value: {test_value})\")\n",
    "\n",
    "print(f\"  Database: ecommerce.db\")\n",
    "print(f\"  Dialect: {engine.dialect.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Create Multi-Table E-Commerce Dataset\n",
    "\n",
    "We'll create a realistic e-commerce database with multiple related tables. This demonstrates the challenge: with many tables, sending all schemas to the LLM becomes impractical.\n",
    "\n",
    "### 2.1 Generate Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Customers table created\n",
      "  Shape: (100, 5)\n",
      "   customer_id        name                  email region signup_date\n",
      "0            1  Customer_1  customer1@example.com   East  2020-01-01\n",
      "1            2  Customer_2  customer2@example.com   West  2020-01-04\n",
      "2            3  Customer_3  customer3@example.com  North  2020-01-07\n",
      "3            4  Customer_4  customer4@example.com   East  2020-01-10\n",
      "4            5  Customer_5  customer5@example.com   East  2020-01-13\n"
     ]
    }
   ],
   "source": [
    "# Create customers table\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate customers\n",
    "num_customers = 100\n",
    "customers_data = {\n",
    "    'customer_id': range(1, num_customers + 1),\n",
    "    'name': [f'Customer_{i}' for i in range(1, num_customers + 1)],\n",
    "    'email': [f'customer{i}@example.com' for i in range(1, num_customers + 1)],\n",
    "    'region': np.random.choice(['North', 'South', 'East', 'West'], num_customers),\n",
    "    'signup_date': pd.date_range('2020-01-01', periods=num_customers, freq='3D')\n",
    "}\n",
    "\n",
    "customers_df = pd.DataFrame(customers_data)\n",
    "print(\"✓ Customers table created\")\n",
    "print(f\"  Shape: {customers_df.shape}\")\n",
    "print(customers_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Products table created\n",
      "  Shape: (50, 5)\n",
      "   product_id product_name  category   price  stock\n",
      "0           1    Product_1  Clothing  486.14     11\n",
      "1           2    Product_2  Clothing  425.97     38\n",
      "2           3    Product_3     Books  363.65      1\n",
      "3           4    Product_4  Clothing  125.63      2\n",
      "4           5    Product_5  Clothing  135.47     55\n"
     ]
    }
   ],
   "source": [
    "# Generate products\n",
    "categories = ['Electronics', 'Clothing', 'Food', 'Books', 'Home']\n",
    "num_products = 50\n",
    "\n",
    "products_data = {\n",
    "    'product_id': range(1, num_products + 1),\n",
    "    'product_name': [f'Product_{i}' for i in range(1, num_products + 1)],\n",
    "    'category': np.random.choice(categories, num_products),\n",
    "    'price': np.round(np.random.uniform(10, 500, num_products), 2),\n",
    "    'stock': np.random.randint(0, 100, num_products)\n",
    "}\n",
    "\n",
    "products_df = pd.DataFrame(products_data)\n",
    "print(\"✓ Products table created\")\n",
    "print(f\"  Shape: {products_df.shape}\")\n",
    "print(products_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Orders table created\n",
      "  Shape: (500, 7)\n",
      "   order_id  customer_id  product_id  quantity order_date     status  \\\n",
      "0         1           67           5         5 2020-01-01  Delivered   \n",
      "1         2           96          20         5 2020-01-02  Delivered   \n",
      "2         3           67          11         4 2020-01-03    Shipped   \n",
      "3         4           27          42         6 2020-01-04  Cancelled   \n",
      "4         5           93           2         3 2020-01-05  Delivered   \n",
      "\n",
      "   order_total  \n",
      "0       677.35  \n",
      "1      1517.20  \n",
      "2      1795.68  \n",
      "3      2095.74  \n",
      "4      1277.91  \n"
     ]
    }
   ],
   "source": [
    "# Generate orders\n",
    "num_orders = 500\n",
    "\n",
    "orders_data = {\n",
    "    'order_id': range(1, num_orders + 1),\n",
    "    'customer_id': np.random.randint(1, num_customers + 1, num_orders),\n",
    "    'product_id': np.random.randint(1, num_products + 1, num_orders),\n",
    "    'quantity': np.random.randint(1, 10, num_orders),\n",
    "    'order_date': pd.date_range('2020-01-01', periods=num_orders, freq='1D'),\n",
    "    'status': np.random.choice(['Pending', 'Shipped', 'Delivered', 'Cancelled'], num_orders, p=[0.1, 0.3, 0.5, 0.1])\n",
    "}\n",
    "\n",
    "orders_df = pd.DataFrame(orders_data)\n",
    "\n",
    "# Add order total based on product price and quantity\n",
    "orders_df = orders_df.merge(products_df[['product_id', 'price']], on='product_id')\n",
    "orders_df['order_total'] = orders_df['quantity'] * orders_df['price']\n",
    "orders_df = orders_df.drop('price', axis=1)\n",
    "\n",
    "print(\"✓ Orders table created\")\n",
    "print(f\"  Shape: {orders_df.shape}\")\n",
    "print(orders_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Reviews table created\n",
      "  Shape: (200, 5)\n",
      "   review_id  product_id  customer_id  rating review_date\n",
      "0          1          34           99       3  2020-06-01\n",
      "1          2          41           40       3  2020-06-03\n",
      "2          3          47           14       3  2020-06-05\n",
      "3          4          18           35       5  2020-06-07\n",
      "4          5          11           48       4  2020-06-09\n"
     ]
    }
   ],
   "source": [
    "# Generate product reviews\n",
    "num_reviews = 200\n",
    "\n",
    "reviews_data = {\n",
    "    'review_id': range(1, num_reviews + 1),\n",
    "    'product_id': np.random.randint(1, num_products + 1, num_reviews),\n",
    "    'customer_id': np.random.randint(1, num_customers + 1, num_reviews),\n",
    "    'rating': np.random.randint(1, 6, num_reviews),\n",
    "    'review_date': pd.date_range('2020-06-01', periods=num_reviews, freq='2D')\n",
    "}\n",
    "\n",
    "reviews_df = pd.DataFrame(reviews_data)\n",
    "print(\"✓ Reviews table created\")\n",
    "print(f\"  Shape: {reviews_df.shape}\")\n",
    "print(reviews_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load Data into Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CSV files created (for Section 5 demo)\n"
     ]
    }
   ],
   "source": [
    "# Save DataFrames as CSV for Section 5 demo (DuckDB can query CSV files directly)\n",
    "customers_df.to_csv('customers.csv', index=False)\n",
    "products_df.to_csv('products.csv', index=False)\n",
    "orders_df.to_csv('orders.csv', index=False)\n",
    "reviews_df.to_csv('reviews.csv', index=False)\n",
    "\n",
    "print(\"✓ CSV files created (for Section 5 demo)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tables loaded into SQLite\n",
      "  Tables: ['customers', 'products', 'orders', 'reviews']\n"
     ]
    }
   ],
   "source": [
    "# Load DataFrames into SQLite using pandas to_sql()\n",
    "with engine.connect() as conn:\n",
    "    customers_df.to_sql('customers', conn, if_exists='replace', index=False)\n",
    "    products_df.to_sql('products', conn, if_exists='replace', index=False)\n",
    "    orders_df.to_sql('orders', conn, if_exists='replace', index=False)\n",
    "    reviews_df.to_sql('reviews', conn, if_exists='replace', index=False)\n",
    "    conn.commit()\n",
    "\n",
    "print(\"✓ Tables loaded into SQLite\")\n",
    "\n",
    "# Verify tables (SQLite syntax)\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(\"SELECT name FROM sqlite_master WHERE type='table'\"))\n",
    "    tables = [row[0] for row in result]\n",
    "    print(f\"  Tables: {tables}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Inspect Schema\n",
    "\n",
    "Now we'll create a SQLDatabase object that LlamaIndex can use to query our tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SQLDatabase created successfully\n",
      "  Tables: customers, products, orders, reviews\n"
     ]
    }
   ],
   "source": [
    "# Create SQLDatabase object\n",
    "sql_database = SQLDatabase(\n",
    "    engine,\n",
    "    include_tables=[\"customers\", \"products\", \"orders\", \"reviews\"]\n",
    ")\n",
    "\n",
    "print(\"✓ SQLDatabase created successfully\")\n",
    "print(\"  Tables: customers, products, orders, reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: ObjectIndex for Table Schemas\n",
    "\n",
    "Now we'll create an ObjectIndex that stores table schemas and allows semantic retrieval.\n",
    "\n",
    "### 3.1 Create SQLTableSchema Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Table schema objects created\n",
      "  Number of tables: 4\n"
     ]
    }
   ],
   "source": [
    "# Define table schemas with descriptive context\n",
    "# The context_str helps the retriever understand when to use each table\n",
    "table_schema_objs = [\n",
    "    SQLTableSchema(\n",
    "        table_name=\"customers\",\n",
    "        context_str=\"\"\"\n",
    "        Contains customer information including customer_id, name, email, region, and signup_date.\n",
    "        Use this table for queries about customer demographics, contact information, \n",
    "        customer segmentation by region, or customer acquisition over time.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    SQLTableSchema(\n",
    "        table_name=\"products\",\n",
    "        context_str=\"\"\"\n",
    "        Contains product catalog with product_id, product_name, category, price, and stock.\n",
    "        Use this table for queries about product information, pricing, inventory levels,\n",
    "        product categories, or available products.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    SQLTableSchema(\n",
    "        table_name=\"orders\",\n",
    "        context_str=\"\"\"\n",
    "        Contains order transactions with order_id, customer_id, product_id, quantity, \n",
    "        order_date, status, and order_total.\n",
    "        Use this table for queries about sales, revenue, order volumes, order status,\n",
    "        purchase patterns, or transaction analysis.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    SQLTableSchema(\n",
    "        table_name=\"reviews\",\n",
    "        context_str=\"\"\"\n",
    "        Contains product reviews with review_id, product_id, customer_id, rating, and review_date.\n",
    "        Use this table for queries about product ratings, customer feedback, \n",
    "        review trends, or product satisfaction analysis.\n",
    "        \"\"\"\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"✓ Table schema objects created\")\n",
    "print(f\"  Number of tables: {len(table_schema_objs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Build Vector Index of Table Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ObjectIndex created\n",
      "  Table schemas are now indexed for semantic retrieval\n"
     ]
    }
   ],
   "source": [
    "# Create node mapping (maps table schemas to actual database tables)\n",
    "table_node_mapping = SQLTableNodeMapping(sql_database)\n",
    "\n",
    "# Build ObjectIndex with VectorStoreIndex\n",
    "# This creates vector embeddings of table descriptions for semantic search\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    table_schema_objs,\n",
    "    table_node_mapping,\n",
    "    VectorStoreIndex,\n",
    ")\n",
    "\n",
    "print(\"✓ ObjectIndex created\")\n",
    "print(\"  Table schemas are now indexed for semantic retrieval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Test Table Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing table retrieval:\n",
      "\n",
      "Query: What is the total revenue?\n",
      "  Retrieved 2 tables\n",
      "    Table 1: retrieved successfully\n",
      "    Table 2: retrieved successfully\n",
      "\n",
      "Query: How many customers do we have?\n",
      "  Retrieved 2 tables\n",
      "    Table 1: retrieved successfully\n",
      "    Table 2: retrieved successfully\n",
      "\n",
      "Query: What are the highest rated products?\n",
      "  Retrieved 2 tables\n",
      "    Table 1: retrieved successfully\n",
      "    Table 2: retrieved successfully\n",
      "\n",
      "Query: Show me products that are out of stock\n",
      "  Retrieved 2 tables\n",
      "    Table 1: retrieved successfully\n",
      "    Table 2: retrieved successfully\n",
      "\n",
      "✓ Table retrieval is working!\n",
      "  The query engine will automatically use the most relevant tables\n"
     ]
    }
   ],
   "source": [
    "# Create a retriever\n",
    "obj_retriever = obj_index.as_retriever(similarity_top_k=2)\n",
    "\n",
    "# Test: What tables are relevant for different queries?\n",
    "test_queries = [\n",
    "    \"What is the total revenue?\",\n",
    "    \"How many customers do we have?\",\n",
    "    \"What are the highest rated products?\",\n",
    "    \"Show me products that are out of stock\"\n",
    "]\n",
    "\n",
    "print(\"Testing table retrieval:\\n\")\n",
    "for query in test_queries:\n",
    "    print(f\"Query: {query}\")\n",
    "    retrieved_objs = obj_retriever.retrieve(query)\n",
    "    print(f\"  Retrieved {len(retrieved_objs)} tables\")\n",
    "    \n",
    "    # Try to show scores if available\n",
    "    for i, obj in enumerate(retrieved_objs, 1):\n",
    "        try:\n",
    "            # Try different ways to access the score\n",
    "            if hasattr(obj, 'score'):\n",
    "                print(f\"    Table {i}: score = {obj.score:.3f}\")\n",
    "            elif hasattr(obj, 'get_score'):\n",
    "                print(f\"    Table {i}: score = {obj.get_score():.3f}\")\n",
    "            else:\n",
    "                print(f\"    Table {i}: retrieved successfully\")\n",
    "        except:\n",
    "            print(f\"    Table {i}: retrieved successfully\")\n",
    "    print()\n",
    "\n",
    "print(\"✓ Table retrieval is working!\")\n",
    "print(\"  The query engine will automatically use the most relevant tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: SQLTableRetrieverQueryEngine\n",
    "\n",
    "Now we'll use the query engine that automatically retrieves relevant tables.\n",
    "\n",
    "### 4.1 Initialize Query Engine with Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SQLTableRetrieverQueryEngine initialized\n",
      "  The engine will automatically retrieve relevant tables for each query\n"
     ]
    }
   ],
   "source": [
    "# Create query engine with automatic table retrieval\n",
    "query_engine = SQLTableRetrieverQueryEngine(\n",
    "    sql_database=sql_database,\n",
    "    table_retriever=obj_retriever,\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"✓ SQLTableRetrieverQueryEngine initialized\")\n",
    "print(\"  The engine will automatically retrieve relevant tables for each query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Query Across Multiple Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the total revenue from all orders?\n",
      "======================================================================\n",
      "> Table Info: Table 'orders' has columns: order_id (BIGINT), customer_id (BIGINT), product_id (BIGINT), quantity (BIGINT), order_date (DATETIME), status (TEXT), order_total (FLOAT), . The table description is: \n",
      "        Contains order transactions with order_id, customer_id, product_id, quantity, \n",
      "        order_date, status, and order_total.\n",
      "        Use this table for queries about sales, revenue, order volumes, order status,\n",
      "        purchase patterns, or transaction analysis.\n",
      "        \n",
      "> Table Info: Table 'products' has columns: product_id (BIGINT), product_name (TEXT), category (TEXT), price (FLOAT), stock (BIGINT), . The table description is: \n",
      "        Contains product catalog with product_id, product_name, category, price, and stock.\n",
      "        Use this table for queries about product information, pricing, inventory levels,\n",
      "        product categories, or available products.\n",
      "        \n",
      "> Table desc str: Table 'orders' has columns: order_id (BIGINT), customer_id (BIGINT), product_id (BIGINT), quantity (BIGINT), order_date (DATETIME), status (TEXT), order_total (FLOAT), . The table description is: \n",
      "        Contains order transactions with order_id, customer_id, product_id, quantity, \n",
      "        order_date, status, and order_total.\n",
      "        Use this table for queries about sales, revenue, order volumes, order status,\n",
      "        purchase patterns, or transaction analysis.\n",
      "        \n",
      "\n",
      "Table 'products' has columns: product_id (BIGINT), product_name (TEXT), category (TEXT), price (FLOAT), stock (BIGINT), . The table description is: \n",
      "        Contains product catalog with product_id, product_name, category, price, and stock.\n",
      "        Use this table for queries about product information, pricing, inventory levels,\n",
      "        product categories, or available products.\n",
      "        \n",
      "> Predicted SQL query: SELECT SUM(order_total) AS total_revenue FROM orders;\n",
      "\n",
      "Answer: The total revenue from all orders is $565,740.69.\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Query 1: Revenue analysis (uses orders table)\n",
    "query = \"What is the total revenue from all orders?\"\n",
    "print(f\"Query: {query}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "response = query_engine.query(query)\n",
    "print(f\"\\nAnswer: {response}\")\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How many customers are in each region?\n",
      "======================================================================\n",
      "> Table Info: Table 'customers' has columns: customer_id (BIGINT), name (TEXT), email (TEXT), region (TEXT), signup_date (DATETIME), . The table description is: \n",
      "        Contains customer information including customer_id, name, email, region, and signup_date.\n",
      "        Use this table for queries about customer demographics, contact information, \n",
      "        customer segmentation by region, or customer acquisition over time.\n",
      "        \n",
      "> Table Info: Table 'orders' has columns: order_id (BIGINT), customer_id (BIGINT), product_id (BIGINT), quantity (BIGINT), order_date (DATETIME), status (TEXT), order_total (FLOAT), . The table description is: \n",
      "        Contains order transactions with order_id, customer_id, product_id, quantity, \n",
      "        order_date, status, and order_total.\n",
      "        Use this table for queries about sales, revenue, order volumes, order status,\n",
      "        purchase patterns, or transaction analysis.\n",
      "        \n",
      "> Table desc str: Table 'customers' has columns: customer_id (BIGINT), name (TEXT), email (TEXT), region (TEXT), signup_date (DATETIME), . The table description is: \n",
      "        Contains customer information including customer_id, name, email, region, and signup_date.\n",
      "        Use this table for queries about customer demographics, contact information, \n",
      "        customer segmentation by region, or customer acquisition over time.\n",
      "        \n",
      "\n",
      "Table 'orders' has columns: order_id (BIGINT), customer_id (BIGINT), product_id (BIGINT), quantity (BIGINT), order_date (DATETIME), status (TEXT), order_total (FLOAT), . The table description is: \n",
      "        Contains order transactions with order_id, customer_id, product_id, quantity, \n",
      "        order_date, status, and order_total.\n",
      "        Use this table for queries about sales, revenue, order volumes, order status,\n",
      "        purchase patterns, or transaction analysis.\n",
      "        \n",
      "> Predicted SQL query: SELECT region, COUNT(customer_id) AS customer_count \n",
      "FROM customers \n",
      "GROUP BY region \n",
      "ORDER BY customer_count DESC;\n",
      "\n",
      "Answer: The number of customers in each region is as follows: \n",
      "\n",
      "- West: 30 customers\n",
      "- South: 26 customers\n",
      "- East: 24 customers\n",
      "- North: 20 customers\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Query 2: Customer segmentation (uses customers table)\n",
    "query = \"How many customers are in each region?\"\n",
    "print(f\"Query: {query}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "response = query_engine.query(query)\n",
    "print(f\"\\nAnswer: {response}\")\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Which product category has the most products?\n",
      "======================================================================\n",
      "> Table Info: Table 'products' has columns: product_id (BIGINT), product_name (TEXT), category (TEXT), price (FLOAT), stock (BIGINT), . The table description is: \n",
      "        Contains product catalog with product_id, product_name, category, price, and stock.\n",
      "        Use this table for queries about product information, pricing, inventory levels,\n",
      "        product categories, or available products.\n",
      "        \n",
      "> Table Info: Table 'reviews' has columns: review_id (BIGINT), product_id (BIGINT), customer_id (BIGINT), rating (BIGINT), review_date (DATETIME), . The table description is: \n",
      "        Contains product reviews with review_id, product_id, customer_id, rating, and review_date.\n",
      "        Use this table for queries about product ratings, customer feedback, \n",
      "        review trends, or product satisfaction analysis.\n",
      "        \n",
      "> Table desc str: Table 'products' has columns: product_id (BIGINT), product_name (TEXT), category (TEXT), price (FLOAT), stock (BIGINT), . The table description is: \n",
      "        Contains product catalog with product_id, product_name, category, price, and stock.\n",
      "        Use this table for queries about product information, pricing, inventory levels,\n",
      "        product categories, or available products.\n",
      "        \n",
      "\n",
      "Table 'reviews' has columns: review_id (BIGINT), product_id (BIGINT), customer_id (BIGINT), rating (BIGINT), review_date (DATETIME), . The table description is: \n",
      "        Contains product reviews with review_id, product_id, customer_id, rating, and review_date.\n",
      "        Use this table for queries about product ratings, customer feedback, \n",
      "        review trends, or product satisfaction analysis.\n",
      "        \n",
      "> Predicted SQL query: SELECT category, COUNT(product_id) AS product_count FROM products GROUP BY category ORDER BY product_count DESC LIMIT 1\n",
      "\n",
      "Answer: The product category with the most products is Electronics, which has a total of 13 products.\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Query 3: Product analysis (uses products table)\n",
    "query = \"Which product category has the most products?\"\n",
    "print(f\"Query: {query}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "response = query_engine.query(query)\n",
    "print(f\"\\nAnswer: {response}\")\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the average product rating?\n",
      "======================================================================\n",
      "> Table Info: Table 'reviews' has columns: review_id (BIGINT), product_id (BIGINT), customer_id (BIGINT), rating (BIGINT), review_date (DATETIME), . The table description is: \n",
      "        Contains product reviews with review_id, product_id, customer_id, rating, and review_date.\n",
      "        Use this table for queries about product ratings, customer feedback, \n",
      "        review trends, or product satisfaction analysis.\n",
      "        \n",
      "> Table Info: Table 'products' has columns: product_id (BIGINT), product_name (TEXT), category (TEXT), price (FLOAT), stock (BIGINT), . The table description is: \n",
      "        Contains product catalog with product_id, product_name, category, price, and stock.\n",
      "        Use this table for queries about product information, pricing, inventory levels,\n",
      "        product categories, or available products.\n",
      "        \n",
      "> Table desc str: Table 'reviews' has columns: review_id (BIGINT), product_id (BIGINT), customer_id (BIGINT), rating (BIGINT), review_date (DATETIME), . The table description is: \n",
      "        Contains product reviews with review_id, product_id, customer_id, rating, and review_date.\n",
      "        Use this table for queries about product ratings, customer feedback, \n",
      "        review trends, or product satisfaction analysis.\n",
      "        \n",
      "\n",
      "Table 'products' has columns: product_id (BIGINT), product_name (TEXT), category (TEXT), price (FLOAT), stock (BIGINT), . The table description is: \n",
      "        Contains product catalog with product_id, product_name, category, price, and stock.\n",
      "        Use this table for queries about product information, pricing, inventory levels,\n",
      "        product categories, or available products.\n",
      "        \n",
      "> Predicted SQL query: SELECT AVG(rating) AS average_rating FROM reviews;\n",
      "\n",
      "Answer: The average product rating is 3.15.\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Query 4: Product reviews (uses reviews table)\n",
    "query = \"What is the average product rating?\"\n",
    "print(f\"Query: {query}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "response = query_engine.query(query)\n",
    "print(f\"\\nAnswer: {response}\")\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Show me the top 5 best-selling products by quantity sold\n",
      "======================================================================\n",
      "> Table Info: Table 'products' has columns: product_id (BIGINT), product_name (TEXT), category (TEXT), price (FLOAT), stock (BIGINT), . The table description is: \n",
      "        Contains product catalog with product_id, product_name, category, price, and stock.\n",
      "        Use this table for queries about product information, pricing, inventory levels,\n",
      "        product categories, or available products.\n",
      "        \n",
      "> Table Info: Table 'orders' has columns: order_id (BIGINT), customer_id (BIGINT), product_id (BIGINT), quantity (BIGINT), order_date (DATETIME), status (TEXT), order_total (FLOAT), . The table description is: \n",
      "        Contains order transactions with order_id, customer_id, product_id, quantity, \n",
      "        order_date, status, and order_total.\n",
      "        Use this table for queries about sales, revenue, order volumes, order status,\n",
      "        purchase patterns, or transaction analysis.\n",
      "        \n",
      "> Table desc str: Table 'products' has columns: product_id (BIGINT), product_name (TEXT), category (TEXT), price (FLOAT), stock (BIGINT), . The table description is: \n",
      "        Contains product catalog with product_id, product_name, category, price, and stock.\n",
      "        Use this table for queries about product information, pricing, inventory levels,\n",
      "        product categories, or available products.\n",
      "        \n",
      "\n",
      "Table 'orders' has columns: order_id (BIGINT), customer_id (BIGINT), product_id (BIGINT), quantity (BIGINT), order_date (DATETIME), status (TEXT), order_total (FLOAT), . The table description is: \n",
      "        Contains order transactions with order_id, customer_id, product_id, quantity, \n",
      "        order_date, status, and order_total.\n",
      "        Use this table for queries about sales, revenue, order volumes, order status,\n",
      "        purchase patterns, or transaction analysis.\n",
      "        \n",
      "> Predicted SQL query: SELECT products.product_name, SUM(orders.quantity) AS total_quantity_sold  \n",
      "FROM orders  \n",
      "JOIN products ON orders.product_id = products.product_id  \n",
      "GROUP BY products.product_name  \n",
      "ORDER BY total_quantity_sold DESC  \n",
      "LIMIT 5;\n",
      "\n",
      "Answer: The top 5 best-selling products by quantity sold are as follows:\n",
      "\n",
      "1. Product_34 - 107 units sold\n",
      "2. Product_29 - 91 units sold\n",
      "3. Product_17 - 90 units sold\n",
      "4. Product_35 - 84 units sold\n",
      "5. Product_45 - 83 units sold\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Query 5: Complex query requiring JOIN\n",
    "query = \"Show me the top 5 best-selling products by quantity sold\"\n",
    "print(f\"Query: {query}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "response = query_engine.query(query)\n",
    "print(f\"\\nAnswer: {response}\")\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Which region has generated the most revenue?\n",
      "======================================================================\n",
      "> Table Info: Table 'customers' has columns: customer_id (BIGINT), name (TEXT), email (TEXT), region (TEXT), signup_date (DATETIME), . The table description is: \n",
      "        Contains customer information including customer_id, name, email, region, and signup_date.\n",
      "        Use this table for queries about customer demographics, contact information, \n",
      "        customer segmentation by region, or customer acquisition over time.\n",
      "        \n",
      "> Table Info: Table 'orders' has columns: order_id (BIGINT), customer_id (BIGINT), product_id (BIGINT), quantity (BIGINT), order_date (DATETIME), status (TEXT), order_total (FLOAT), . The table description is: \n",
      "        Contains order transactions with order_id, customer_id, product_id, quantity, \n",
      "        order_date, status, and order_total.\n",
      "        Use this table for queries about sales, revenue, order volumes, order status,\n",
      "        purchase patterns, or transaction analysis.\n",
      "        \n",
      "> Table desc str: Table 'customers' has columns: customer_id (BIGINT), name (TEXT), email (TEXT), region (TEXT), signup_date (DATETIME), . The table description is: \n",
      "        Contains customer information including customer_id, name, email, region, and signup_date.\n",
      "        Use this table for queries about customer demographics, contact information, \n",
      "        customer segmentation by region, or customer acquisition over time.\n",
      "        \n",
      "\n",
      "Table 'orders' has columns: order_id (BIGINT), customer_id (BIGINT), product_id (BIGINT), quantity (BIGINT), order_date (DATETIME), status (TEXT), order_total (FLOAT), . The table description is: \n",
      "        Contains order transactions with order_id, customer_id, product_id, quantity, \n",
      "        order_date, status, and order_total.\n",
      "        Use this table for queries about sales, revenue, order volumes, order status,\n",
      "        purchase patterns, or transaction analysis.\n",
      "        \n",
      "> Predicted SQL query: SELECT c.region, SUM(o.order_total) AS total_revenue FROM customers c JOIN orders o ON c.customer_id = o.customer_id GROUP BY c.region ORDER BY total_revenue DESC LIMIT 1;\n",
      "\n",
      "Answer: The region that has generated the most revenue is the West, with a total revenue of $159,737.54.\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Query 6: Multi-table analysis\n",
    "query = \"Which region has generated the most revenue?\"\n",
    "print(f\"Query: {query}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "response = query_engine.query(query)\n",
    "print(f\"\\nAnswer: {response}\")\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Compare with Direct Table Specification\n",
    "\n",
    "Let's compare the automatic retrieval approach with manually specifying tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual approach: All table schemas sent to LLM every time\n",
      "Automatic approach: Only relevant tables retrieved per query\n",
      "\n",
      "For 4 tables, the difference is small.\n",
      "For 50+ tables, automatic retrieval is essential!\n"
     ]
    }
   ],
   "source": [
    "# Manual approach (from Notebook 1)\n",
    "from llama_index.core.query_engine import NLSQLTableQueryEngine\n",
    "\n",
    "# Must specify all tables upfront\n",
    "manual_query_engine = NLSQLTableQueryEngine(\n",
    "    sql_database=sql_database,\n",
    "    tables=[\"customers\", \"products\", \"orders\", \"reviews\"],  # All tables\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "print(\"Manual approach: All table schemas sent to LLM every time\")\n",
    "print(\"Automatic approach: Only relevant tables retrieved per query\")\n",
    "print(\"\\nFor 4 tables, the difference is small.\")\n",
    "print(\"For 50+ tables, automatic retrieval is essential!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Bonus - DuckDB for Direct CSV Queries\n",
    "\n",
    "### 5.1 Why DuckDB?\n",
    "\n",
    "**DuckDB** is an in-process SQL OLAP database perfect for analytical workloads. While we use SQLite for LlamaIndex compatibility, DuckDB excels at:\n",
    "- Querying CSV/Parquet files directly (no loading required!)\n",
    "- Fast analytical queries\n",
    "- OLAP workloads\n",
    "- Data analysis pipelines\n",
    "\n",
    "Let's demonstrate DuckDB's powerful CSV querying capabilities!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ DuckDB engine created for CSV queries\n",
      "  DuckDB can query CSV files without loading them into tables!\n"
     ]
    }
   ],
   "source": [
    "# Create a DuckDB engine for CSV queries\n",
    "duckdb_engine = create_engine(\"duckdb:///:memory:\")\n",
    "\n",
    "print(\"✓ DuckDB engine created for CSV queries\")\n",
    "print(\"  DuckDB can query CSV files without loading them into tables!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Query CSV Files Directly with DuckDB\n",
    "\n",
    "DuckDB's killer feature: query CSV files as if they were tables!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query results from CSV (no table loading required!):\n",
      "\n",
      "  Electronics: 13 products, avg price $255.31\n",
      "  Books: 10 products, avg price $216.78\n",
      "  Clothing: 9 products, avg price $277.04\n",
      "  Food: 9 products, avg price $177.37\n",
      "  Home: 9 products, avg price $195.58\n"
     ]
    }
   ],
   "source": [
    "# DuckDB can query CSV files without loading them!\n",
    "with duckdb_engine.connect() as conn:\n",
    "    result = conn.execute(text(\"\"\"\n",
    "        SELECT \n",
    "            category,\n",
    "            COUNT(*) as product_count,\n",
    "            ROUND(AVG(price), 2) as avg_price\n",
    "        FROM read_csv_auto('products.csv')\n",
    "        GROUP BY category\n",
    "        ORDER BY product_count DESC\n",
    "    \"\"\"))\n",
    "    \n",
    "    print(\"Query results from CSV (no table loading required!):\")\n",
    "    print()\n",
    "    for row in result:\n",
    "        print(f\"  {row[0]}: {row[1]} products, avg price ${row[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Complex Analytical Queries with DuckDB\n",
    "\n",
    "DuckDB is optimized for analytical queries. Let's run a complex multi-table JOIN directly on CSV files!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Query execution time: 39.79 ms\n",
      "  (Querying 2 CSV files with JOIN - no table loading!)\n",
      "\n",
      "Results:\n",
      "      category  order_count  total_revenue  avg_order_value\n",
      "0  Electronics          134      169412.22          1264.27\n",
      "1     Clothing           85      127252.07          1497.08\n",
      "2        Books          108      120766.69          1118.21\n",
      "3         Home           96       90907.28           946.95\n",
      "4         Food           77       57402.43           745.49\n"
     ]
    }
   ],
   "source": [
    "# Complex analytical query joining multiple CSV files\n",
    "import time\n",
    "\n",
    "analytical_query = \"\"\"\n",
    "SELECT \n",
    "    p.category,\n",
    "    COUNT(DISTINCT o.order_id) as order_count,\n",
    "    SUM(o.order_total) as total_revenue,\n",
    "    ROUND(AVG(o.order_total), 2) as avg_order_value\n",
    "FROM read_csv_auto('orders.csv') o\n",
    "JOIN read_csv_auto('products.csv') p ON o.product_id = p.product_id\n",
    "GROUP BY p.category\n",
    "ORDER BY total_revenue DESC\n",
    "\"\"\"\n",
    "\n",
    "start = time.time()\n",
    "with duckdb_engine.connect() as conn:\n",
    "    result = pd.read_sql(analytical_query, conn)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"✓ Query execution time: {(end - start) * 1000:.2f} ms\")\n",
    "print(f\"  (Querying 2 CSV files with JOIN - no table loading!)\\n\")\n",
    "print(\"Results:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 When to Use SQLite vs DuckDB\n",
    "\n",
    "**Use SQLite when:**\n",
    "- Need LlamaIndex SQLDatabase integration\n",
    "- Write-heavy workloads\n",
    "- Embedded applications\n",
    "- Need ACID guarantees\n",
    "- Smaller datasets\n",
    "\n",
    "**Use DuckDB when:**\n",
    "- Analytical queries (aggregations, complex JOINs)\n",
    "- Querying CSV/Parquet files directly\n",
    "- Large datasets\n",
    "- Read-heavy workloads\n",
    "- Data analysis pipelines\n",
    "- Want to avoid loading data into tables\n",
    "\n",
    "**Best Practice:** Use SQLite for text-to-SQL with Llama Index (better compatibility), and DuckDB for direct data analysis on files!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Exercises\n",
    "\n",
    "### Exercise 1: Basic Queries\n",
    "1. How many products are currently out of stock?\n",
    "2. What is the total number of orders by status?\n",
    "3. Which customer has placed the most orders?\n",
    "\n",
    "### Exercise 2: Table Retrieval Analysis\n",
    "1. Test the table retriever with different queries\n",
    "2. Observe which tables are retrieved for each query type\n",
    "3. Try queries that might require multiple tables\n",
    "\n",
    "### Exercise 3: Complex Analytics\n",
    "1. Find the most profitable product category\n",
    "2. Calculate customer lifetime value by region\n",
    "3. Identify products with high ratings but low sales\n",
    "\n",
    "### Exercise 4: DuckDB Exploration\n",
    "1. Write a DuckDB query to join all 4 CSV files\n",
    "2. Compare query performance between SQLite (loaded tables) vs DuckDB (CSV files)\n",
    "3. Find use cases where DuckDB's direct CSV querying would be beneficial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "✓ Working with multi-table databases  \n",
    "✓ Dynamic table retrieval with `ObjectIndex`  \n",
    "✓ `SQLTableRetrieverQueryEngine` for automatic table selection  \n",
    "✓ When to use dynamic retrieval vs direct specification  \n",
    "✓ DuckDB's powerful CSV querying capabilities  \n",
    "✓ Choosing between SQLite and DuckDB for different use cases\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Scalability**: Dynamic table retrieval is essential for databases with many tables\n",
    "2. **Context matters**: Good table descriptions improve retrieval accuracy\n",
    "3. **Database choice**: SQLite for LlamaIndex compatibility, DuckDB for direct file queries\n",
    "4. **Vector search**: Semantic similarity helps find relevant tables\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue to **Notebook 03: Advanced Text-to-SQL with Workflows** to learn:\n",
    "- LlamaIndex Workflows architecture\n",
    "- Query-time row retrieval\n",
    "- Production-ready patterns\n",
    "- Advanced error handling\n",
    "\n",
    "---\n",
    "\n",
    "**Excellent work!** You're now ready for advanced topics. 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
